\documentclass{article}
\usepackage{authblk}
\usepackage{mathptmx}
\usepackage{url,latexsym,amsmath,amsthm,xspace,rotating,multirow,multicol,xspace,amssymb,paralist}
\usepackage{euscript}
\usepackage{fancybox,xcolor}
\usepackage{longtable}
\usepackage{paralist}
\usepackage[normalem]{ulem}
\usepackage[pdftex]{hyperref}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{cancel}
\usepackage{mathtools}

\usepackage{url}
\usepackage{latexsym}

\usepackage{times}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
%\usepackage{hyperref}
\usepackage{url}
%\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{booktabs}
\usepackage{color}
\usepackage[normalem]{ulem}

\newcommand{\obs}{\text{obs}}
\newcommand{\mis}{\text{mis}}

\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\ql}[1]{\left[#1\right]}
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\hl}{HL}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\text{softmax}}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}
\newcommand{\f}{\text{f}}
%\newcommand{\log}{\text{log}}

\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\std}[0]{\operatorname{std}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\ve}[0]{\vect{e}}

\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vp}[0]{\vect{p}}
\newcommand{\vz}[0]{\vect{z}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vi}[0]{\vect{i}}
\newcommand{\vo}[0]{\vect{o}}
\newcommand{\vd}[0]{\vect{d}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\vr}[0]{\vect{r}}
\newcommand{\vone}[0]{\vect{1}}

\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mE}[0]{\matr{E}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mY}[0]{\matr{Y}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mF}[0]{\matr{F}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mC}{\matr{C}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mL}[0]{\matr{L}}
\newcommand{\mR}[0]{\matr{R}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}
\newcommand{\td}[0]{\text{d}}
\newcommand{\TT}[0]{\vects{\theta}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vzero}[0]{\vect{0}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
\newcommand{\YY}[0]{\mathcal{Y}}
\newcommand{\BB}[0]{\mathcal{B}}
\newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\MM}[0]{\mathcal{M}}
\newcommand{\OO}[0]{\mathbb{O}}
\newcommand{\II}[0]{\mathbb{I}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\sign}{\text{sign}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}
\newcommand{\enhnabla}[0]{\nabla_{\hspace{-0.5mm}e}\,}
\newcommand{\eos}[0]{\ensuremath{\left< \text{eos}\right>}}


\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}
\newcommand{\done}[1]{{\Large\textcolor{green}{#1}}}
\newcommand{\dd}[1]{\ensuremath{\mbox{d}#1}}

\DeclareMathOperator*{\argmax}{\arg \max}
\DeclareMathOperator*{\argmin}{\arg \min}
\newcommand{\newln}{\\&\quad\quad{}}

\newcommand{\BP}{\text{BP}}
\newcommand{\PPL}{\text{PPL}}
\newcommand{\PL}{\text{PL}}
\newcommand{\MatSum}{\text{MatSum}}
\newcommand{\MatMul}{\text{MatMul}}
\newcommand{\KL}{\text{KL}}
\newcommand{\data}{\text{data}}
\newcommand{\rect}{\text{rect}}
\newcommand{\maxout}{\text{maxout}}
\newcommand{\train}{\text{train}}
\newcommand{\hinge}{\text{hinge}}
\newcommand{\val}{\text{val}}
\newcommand{\init}{\text{init}}
\newcommand{\fenc}{\text{fenc}}
\newcommand{\renc}{\text{renc}}
\newcommand{\enc}{\text{enc}}
\newcommand{\dec}{\text{dec}}
\newcommand{\test}{\text{test}}
\newcommand{\tra}{\text{tra}}
\newcommand{\Ax}{\mathcal{A}_x}
\newcommand{\Ay}{\mathcal{A}_y}
\newcommand{\ola}{\overleftarrow}
\newcommand{\ora}{\overrightarrow}
\newcommand{\ov}{\overline}
\newcommand{\ts}{\rule{0pt}{2.6ex}}       % Top strut
\newcommand{\ms}{\rule{0pt}{0ex}}         % Middle strut
\newcommand{\bs}{\rule[-1.2ex]{0pt}{0pt}} % Bottom strut
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}


%\usepackage{bibentry}
%\nobibliography*

\begin{document}

\title{Homework Assignment 1 \\
Perceptron and Logistic Regression}
\author{Lecturer: Kyunghyun Cho}
%\affil{
%    Courant Institute of Mathematical Sciences and \\
%    Center for Data Science,\\
%    New York University 
%}

\maketitle
\pagenumbering{arabic}

\paragraph{Submission Instruction} 

You must typeset the answers to the theory questions using LaTeX or Microsoft
Word and compile them into a single PDF file. For the programming assignment,
complete the two Jupyter notebooks.  Create a
ZIP file containing both the PDF file and the two completed Jupyter notebooks,
and name it ``$\left<\text{Your-NetID}\right>$\_hw1.zip''. Email this file to
\url{intro2ml@gmail.com} within two weeks since the announcement of the
homework.


\paragraph{1.} When defining a perceptron, we have augmented an input vector
$\vx$ with an extra $1$: 
\begin{align*}
    M(\vx) = \sign(\vw^\top \tilde{\vx}),
\end{align*}
where $\tilde{\vx} = \left[ \vx; 1\right]$. Why is this necessary? Provide an
example in which this extra $1$ is necessary.
\\
\\
SOLUTION: The model equation created after the learning process is supposed to provide a line that classifies inputs as accurately as possible. The number of variables is equivalent to $d$, or the dimension of $\vx$, and each coefficient corresponds to the values in $\vw$. With such an equation, there is nowhere to place a constant to account for a vertical shift in the model: it only permits models which pass through the origin. In some cases, the optimal solution after learning may be a model that does not pass through the origin, but somewhere else along the y axis. By appending a 1 to the end of $\vx$, we can obtain an additional value from $\vw$ during learning, which ends up being the bias. This "bias" helps shift the model by a constant and can thus provide a more optimal solution. 

\vspace{5mm}

\paragraph{2.} We used the following distance function for perceptron in the
lecture:
\begin{align*}
    D(M^*(\vx), M, \vx) = -\left( M^*(\vx) - M(\vx)
    \right) \left(\vw^\top
    \tilde{\vx}\right).
\end{align*}
This distance function has a problem of a trivial solution. What is the trivial
solution? Propose a solution to this.

\vspace{5mm}

\paragraph{3.} The distance function of logistic regression was defined as 
\begin{align*}
    D(y^*, \vw, \vx) = -(y^* \log M(\vx) + (1-y^*) \log (1- M(\vx))).
\end{align*}
Derive its gradient with respect to the weight vector $\vw$ step-by-step.
\\
\\
SOLUTION: 
Let $a = -y^* \log M(\vx)$ and $b =  \log (1- M(\vx))$ \\
We also know that $M(\vx) = \sigmoid(\vw^\top \tilde{\vx}) $ \\
Based off this, we can say $a = -y^* \log ( \frac{1}{1+e^{-\vw^{\top \tilde{\vx}}}})$ and $b =  \log (1- \frac{1}{1+e^{-\vw^\top \tilde{\vx}}})$ \\
$b = \log (1- \frac{1}{1+e^{-\vw^\top \tilde{\vx}}}) = \log ( \frac{1+e^{-\vw^\top \tilde{\vx} } - 1}{1+e^{-\vw^\top \tilde{\vx}}}) = \log ( \frac{e^{-\vw^\top \tilde{\vx} }}{1+e^{-\vw^\top \tilde{\vx}}}) =  \log ( e^{-\vw^\top \tilde{\vx} } ) - \log (1+e^{-\vw^\top \tilde{\vx}}) $ \\
$ \frac{db}{d \vw^\top} = \frac{-\tilde{\vx} e^{-\vw^\top \tilde{\vx}}}{e^{-\vw^\top \tilde{\vx}}} + \frac{\tilde{\vx} e^{-\vw^\top \tilde{\vx}}}{1 + e^{-\vw^\top \tilde{\vx}}}  = \frac{-\tilde{\vx} (1 + e^{-\vw^\top \tilde{\vx}}) +\tilde{\vx} e^{-\vw^\top \tilde{\vx}}}{1 + e^{-\vw^\top \tilde{\vx}}}  = \frac{\tilde{\vx}}{1 + e^{-\vw^\top \tilde{\vx}}}$ \\
$ \frac{da}{d \vw^\top} = \frac {-y^* \tilde{\vx}e^{-\vw^\top \tilde{\vx}}}{1 + e^{-\vw^\top \tilde{\vx}}}$ \\
Bringing this together: \\
$ \nabla_{\vw} D(M^*(\vx), M, \vx) =  \frac {-y^* \tilde{\vx}e^{-\vw^\top \tilde{\vx}}}{1 + e^{-\vw^\top \tilde{\vx}}} - \frac{y^* \tilde{\vx}}{1 + e^{-\vw^\top \tilde{\vx}}} + \frac{\tilde{\vx}}{1 + e^{-\vw^\top \tilde{\vx}}} $ \\
$ = -y^* \tilde{\vx}e^{-\vw^\top \tilde{\vx}} \sigmoid(\vw^\top \tilde{\vx}) -y^* \tilde{\vx}\sigmoid(\vw^\top \tilde{\vx}) + \tilde{\vx}\sigmoid(\vw^\top \tilde{\vx}) = \tilde{\vx}\sigmoid(\vw^\top \tilde{\vx}) (-y^* (1 + e^{-\vw^\top \tilde{\vx}}) + 1)$ \\
$ = \tilde{\vx}\sigmoid(\vw^\top \tilde{\vx}) (\frac{-y^* }{\sigmoid(\vw^\top \tilde{\vx})} + 1) = \tilde{\vx} (-y^* + \sigmoid(\vw^\top \tilde{\vx}))$ \\
$ = -(M^*(\vx) - M(\vx)) \tilde{\vx}$


\vspace{5mm}



\paragraph{4.} (Programming Assignment)
Complete the implementation of perceptron and logistic regression using Python
and scikit-learn. The completed notebooks must be submitted together with the
answers to the questions above. 
\begin{itemize}
    \item[Perceptron]
        \url{https://github.com/nyu-dl/Intro_to_ML_Lecture_Note/blob/master/notebook/Perceptron1.ipynb}
    \item[Logistic Regression]
        \url{https://github.com/nyu-dl/Intro_to_ML_Lecture_Note/blob/master/notebook/Logistic%20Regression%201.ipynb}
\end{itemize}





%\bibliographystyle{abbrv}
%\bibliography{../lecture_note}


\end{document}






