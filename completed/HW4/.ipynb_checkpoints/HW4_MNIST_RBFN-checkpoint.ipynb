{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI-UA 0473 - Introduction to Machine Learning\n",
    "## Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task for this homework is again that of handwritten digit recognition on the MNIST dataset. From the huge corpus of 70k images, I have sampled 3k images uniformly i.e. there are 300 images in each class.\n",
    "\n",
    "I have split the 3k samples into training set (2500 points) and test set (500 points) randomly. This experimental dataset is the same for all and I have provided the pickle files with the corresponding data.\n",
    "\n",
    "In this homework, you will be experimenting with different distance functions and number of bases used for the RBF Network. No complicated (or simple, for some) coding involved. Only play with the distance function and the number of bases.\n",
    "\n",
    "Conduct your experiments in a principled way to decide the best distance function according to you and the optimal value of number of bases for the best distance function. Use the accuracy score as the evaluation metric.\n",
    "\n",
    "Explain your approach clearly in the write-up.\n",
    "\n",
    "Report your experiment results using plots (preferably) or tables with the accuracies in the write-up.\n",
    "\n",
    "(EXTRA CREDITS) Come up with your own distance function, explain it clearly and provide the results from your distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import scipy.optimize\n",
    "import scipy.spatial.distance\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training Data: ', (2500, 784))\n",
      "('Test Data: ', (500, 784))\n",
      "('Training Label Distribution: ', array([249, 253, 253, 246, 249, 248, 245, 244, 261, 252]))\n",
      "('Test Label Distribution: ', array([51, 47, 47, 54, 51, 52, 55, 56, 39, 48]))\n"
     ]
    }
   ],
   "source": [
    "training_data = pickle.load(open('training_data_hw4.p', 'rb'))\n",
    "training_labels = pickle.load(open('training_labels_hw4.p', 'rb'))\n",
    "\n",
    "test_data = pickle.load(open('test_data_hw4.p', 'rb'))\n",
    "test_labels = pickle.load(open('test_labels_hw4.p', 'rb'))\n",
    "\n",
    "\n",
    "print ('Training Data: ', training_data.shape)\n",
    "print ('Test Data: ', test_data.shape)\n",
    "\n",
    "print ('Training Label Distribution: ', np.bincount(training_labels))\n",
    "print ('Test Label Distribution: ', np.bincount(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RBF - Experiment Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rbf(x, bases, width=1., metric=\"euclidean\"):\n",
    "#     return np.exp(-(((x[:,None,:] - bases[None,:,:]) ** 2).sum(-1) / width))\n",
    "    return np.exp(-scipy.spatial.distance.cdist(x, bases, metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep track of this variable for your computations. It's a global variable and it's value \n",
    "# will be reflected in many functions.\n",
    "\n",
    "n_bases = 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Definition of the multinomial logistic regression model.\n",
    "\n",
    "INPUT: Feature vector (x) and weight matrix (w)\n",
    "OUTPUT: The probability of each data point belonging to each class. If you have 'm' data points and 'k' classes, this \n",
    "        function should return a matrix of dimension (m X k) with values in each row summing to 1, as per definition.\n",
    "'''\n",
    "\n",
    "def multinomial_logreg(x, w):  \n",
    "     \n",
    "    y = np.dot(x, w.reshape(n_bases, 10))\n",
    "    \n",
    "    y_ = np.exp(y)\n",
    "    \n",
    "    return y_ / y_.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Distance function of the multinomial logistic regression model (popularly called cross-entropy loss). \n",
    "\n",
    "INPUT: True labels (y), feature vector (x) and weight vector (w)\n",
    "OUTPUT: Log of the likelihood for the given 'w'\n",
    "'''\n",
    "\n",
    "def multinomial_lr_distance(y, x, w):\n",
    "    y_pred = multinomial_logreg(x, w)\n",
    "    \n",
    "    distance = -(np.mean([y_pred[i, y[i]] for i in range(x.shape[0])]))\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(w, x, y):\n",
    "    return multinomial_lr_distance(y, x, w)\n",
    "\n",
    "# Computing the gradient\n",
    "multinomial_lr_rule = grad(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _multinomial_lr_dist(w, x, y):\n",
    "    return multinomial_lr_distance(y, x, w), multinomial_lr_rule(w, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select random centroids\n",
    "cids = np.random.permutation(len(training_data))[:n_bases]\n",
    "all_centroids = training_data[cids]\n",
    "\n",
    "x_tra_ = rbf(training_data, all_centroids, 1.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 1800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tra_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 10)\n"
     ]
    }
   ],
   "source": [
    "w0 = 0.01 * np.random.randn(n_bases, 10); w0[:, -1] = 0.\n",
    "print w0.shape\n",
    "w = np.copy(w0)\n",
    "\n",
    "trained_model = scipy.optimize.minimize(_multinomial_lr_dist, w0, (x_tra_, training_labels), method='L-BFGS-B', jac=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_learned = trained_model.x.reshape((n_bases, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_learned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Play Area - Different Distance Functions and Number of Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = multinomial_logreg(x_tra_, w_learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (Euclidean):  0.7536\n"
     ]
    }
   ],
   "source": [
    "print 'Train Accuracy (Euclidean): ', np.sum(training_labels == np.argmax(y_train_pred, axis = 1)) * 1. / training_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rossfreeman/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:15: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine\n"
     ]
    }
   ],
   "source": [
    "# WRITE CODE TO FIND THE TEST ACCURACY USING YOUR EXPERIMENTAL SETUP\n",
    "functions = [\"braycurtis\", \"canberra\", \"chebyshev\", \"cityblock\", \"correlation\", \"cosine\", \"dice\", \"euclidean\", \"hamming\", \"jaccard\", \"kulsinski\", \"matching\", \"minkowski\", \"rogerstanimoto\", \"russellrao\", \"seuclidean\", \"sokalmichener\", \"sokalsneath\", \"sqeuclidean\", \"yule\"]\n",
    "\n",
    "max_val = 0\n",
    "max_fun = \"\"\n",
    "k = 1\n",
    "\n",
    "for func in functions:\n",
    "    x_tra_ = rbf(training_data, all_centroids, 1., func) \n",
    "    trained_model = scipy.optimize.minimize(_multinomial_lr_dist, w0, (x_tra_, training_labels), method='L-BFGS-B', jac=True)\n",
    "    w_learned = trained_model.x.reshape((n_bases, 10))\n",
    "    y_train_pred = multinomial_logreg(x_tra_, w_learned)\n",
    "    \n",
    "    tes_acc = np.sum(training_labels == np.argmax(y_train_pred, axis = 1)) * 1. / training_data.shape[0]\n",
    "    if(tes_acc > max_val):\n",
    "        max_val = tes_acc\n",
    "        max_fun = func\n",
    "print(max_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rossfreeman/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:15: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "max_val = 0\n",
    "max_k = 0\n",
    "acc = dict()\n",
    "\n",
    "for k in xrange(1900, 1950, 10):\n",
    "    n_bases = k\n",
    "    \n",
    "    cids = np.random.permutation(len(training_data))[:n_bases]\n",
    "    all_centroids = training_data[cids]\n",
    "    \n",
    "    w0 = 0.01 * np.random.randn(n_bases, 10); w0[:, -1] = 0.\n",
    "    w = np.copy(w0)\n",
    "    \n",
    "    x_tra_ = rbf(training_data, all_centroids, 1., max_fun) \n",
    "    trained_model = scipy.optimize.minimize(_multinomial_lr_dist, w0, (x_tra_, training_labels), method='L-BFGS-B', jac=True)\n",
    "    w_learned = trained_model.x.reshape((n_bases, 10))\n",
    "    x_test = rbf(test_data, all_centroids, 1., max_fun)\n",
    "    y_test_pred = multinomial_logreg(x_test, w_learned)\n",
    "    \n",
    "    tes_acc = np.sum(test_labels == np.argmax(y_test_pred, axis = 1)) * 1. / test_data.shape[0]\n",
    "    acc[k] = tes_acc\n",
    "    if(tes_acc > max_val):\n",
    "        max_val = tes_acc\n",
    "        max_k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1920: 0.078, 1940: 0.38600000000000001, 1930: 0.19600000000000001, 1900: 0.38, 1910: 0.19400000000000001}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGINfkjrG4JekjjH4JaljDH5J6pj/DyeWRLHBcBywAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113400950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(acc)\n",
    "plot.scatter(acc.keys(), acc.values())\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.266\n"
     ]
    }
   ],
   "source": [
    "print(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
