{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI-UA 0473 - Introduction to Machine Learning\n",
    "## Wednesday, March 1, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last class, you saw about using multinomial logistic regression in sklearn. In this programming assignment, you will be implementing multinomial logistic regression using numpy. \n",
    "\n",
    "The required libraries are imported for you and they are sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from autograd import grad\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and understanding, let's restrict to just 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample dataset preparation\n",
    "\n",
    "n_dim = 2\n",
    "x_train, y_train = make_blobs(n_samples=200, n_features=n_dim, centers=[[2,2],[0,-3], [-2, 2]], shuffle=True)\n",
    "x_test, y_test = make_blobs(n_samples=100, n_features=n_dim, centers=[[2,2],[0,-3], [-2, 2]], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adds 1 at the end of each data vector in both training and test data\n",
    "\n",
    "x_train = np.insert(x_train, 2, 1, axis=1)\n",
    "x_test = np.insert(x_test, 2, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67, 67, 66])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class distribution\n",
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Definition of the multinomial logistic regression model.\n",
    "\n",
    "INPUT: Feature vector (x) and weight matrix (w)\n",
    "OUTPUT: The probability of each data point belonging to each class. If you have 'm' data points and 'k' classes, this \n",
    "        function should return a matrix of dimension (m X k) with values in each row summing to 1, as per definition.\n",
    "'''\n",
    "\n",
    "def multinomial_logreg(x, w):  \n",
    "     \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Distance function of the multinomial logistic regression model (popularly called cross-entropy loss). \n",
    "\n",
    "INPUT: True labels (y), feature vector (x) and weight vector (w)\n",
    "OUTPUT: Log of the likelihood for the given 'w'\n",
    "'''\n",
    "\n",
    "def multinomial_lr_distance(y, x, w):\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(w, x, y):\n",
    "    return multinomial_lr_distance(y, x, w)\n",
    "\n",
    "# Computing the gradient\n",
    "multinomial_lr_rule = grad(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _multinomial_lr_dist(w, x, y):\n",
    "    return multinomial_lr_distance(y, x, w), multinomial_lr_rule(w, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random starting point created for you\n",
    "w0 = 0.01 * np.random.randn(3, 3); w0[:, -1] = 0.\n",
    "w = np.copy(w0)\n",
    "\n",
    "# TO DO: Use scipy.optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TO DO: Print the learned weight matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO: Predict the class for test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = ?\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Calculate the accuracy of test predictions\n",
    "\n",
    "print 'Test Accuracy = ?',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
