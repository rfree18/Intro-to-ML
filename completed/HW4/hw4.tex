\documentclass{article}
\usepackage{authblk}
\usepackage{mathptmx}
\usepackage{url,latexsym,amsmath,amsthm,xspace,rotating,multirow,multicol,xspace,amssymb,paralist}
\usepackage{euscript}
\usepackage{fancybox,xcolor}
\usepackage{longtable}
\usepackage{paralist}
\usepackage[normalem]{ulem}
\usepackage[pdftex]{hyperref}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{cancel}
\usepackage{mathtools}

\usepackage{url}
\usepackage{latexsym}

\usepackage{times}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
%\usepackage{hyperref}
\usepackage{url}
%\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{booktabs}
\usepackage{color}
\usepackage[normalem]{ulem}

\newcommand{\obs}{\text{obs}}
\newcommand{\mis}{\text{mis}}

\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\ql}[1]{\left[#1\right]}
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\hl}{HL}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\text{softmax}}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}
\newcommand{\f}{\text{f}}
%\newcommand{\log}{\text{log}}

\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\std}[0]{\operatorname{std}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\ve}[0]{\vect{e}}

\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vp}[0]{\vect{p}}
\newcommand{\vz}[0]{\vect{z}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vi}[0]{\vect{i}}
\newcommand{\vo}[0]{\vect{o}}
\newcommand{\vd}[0]{\vect{d}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\vr}[0]{\vect{r}}
\newcommand{\vone}[0]{\vect{1}}

\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mE}[0]{\matr{E}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mY}[0]{\matr{Y}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mF}[0]{\matr{F}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mC}{\matr{C}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mL}[0]{\matr{L}}
\newcommand{\mR}[0]{\matr{R}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}
\newcommand{\td}[0]{\text{d}}
\newcommand{\TT}[0]{\vects{\theta}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vzero}[0]{\vect{0}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
\newcommand{\YY}[0]{\mathcal{Y}}
\newcommand{\BB}[0]{\mathcal{B}}
\newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\MM}[0]{\mathcal{M}}
\newcommand{\OO}[0]{\mathbb{O}}
\newcommand{\II}[0]{\mathbb{I}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\sign}{\text{sign}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}
\newcommand{\enhnabla}[0]{\nabla_{\hspace{-0.5mm}e}\,}
\newcommand{\eos}[0]{\ensuremath{\left< \text{eos}\right>}}


\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}
\newcommand{\done}[1]{{\Large\textcolor{green}{#1}}}
\newcommand{\dd}[1]{\ensuremath{\mbox{d}#1}}

\DeclareMathOperator*{\argmax}{\arg \max}
\DeclareMathOperator*{\argmin}{\arg \min}
\newcommand{\newln}{\\&\quad\quad{}}

\newcommand{\BP}{\text{BP}}
\newcommand{\PPL}{\text{PPL}}
\newcommand{\PL}{\text{PL}}
\newcommand{\MatSum}{\text{MatSum}}
\newcommand{\MatMul}{\text{MatMul}}
\newcommand{\KL}{\text{KL}}
\newcommand{\data}{\text{data}}
\newcommand{\rect}{\text{rect}}
\newcommand{\maxout}{\text{maxout}}
\newcommand{\train}{\text{train}}
\newcommand{\hinge}{\text{hinge}}
\newcommand{\val}{\text{val}}
\newcommand{\init}{\text{init}}
\newcommand{\fenc}{\text{fenc}}
\newcommand{\renc}{\text{renc}}
\newcommand{\enc}{\text{enc}}
\newcommand{\dec}{\text{dec}}
\newcommand{\test}{\text{test}}
\newcommand{\tra}{\text{tra}}
\newcommand{\Ax}{\mathcal{A}_x}
\newcommand{\Ay}{\mathcal{A}_y}
\newcommand{\ola}{\overleftarrow}
\newcommand{\ora}{\overrightarrow}
\newcommand{\ov}{\overline}
\newcommand{\ts}{\rule{0pt}{2.6ex}}       % Top strut
\newcommand{\ms}{\rule{0pt}{0ex}}         % Middle strut
\newcommand{\bs}{\rule[-1.2ex]{0pt}{0pt}} % Bottom strut
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}


%\usepackage{bibentry}
%\nobibliography*

\begin{document}

\title{Homework Assignment 4}
\author{Lecturer: Kyunghyun Cho}
%\affil{
%    Courant Institute of Mathematical Sciences and \\
%    Center for Data Science,\\
%    New York University 
%}

\maketitle
\pagenumbering{arabic}

\paragraph{1.} 

(a) For the 2-dimensional XOR problem, we select the following four basis
vectors:
\begin{align*}
    &\vr^1 = [-1, -1]^\top \\
    &\vr^2 = [1, 1]^\top \\
    &\vr^3 = [-1, 1]^\top \\
    &\vr^4 = [1, -1]^\top.
\end{align*}
Show that the XOR problem is solved by the radial basis function network with
the following weight vector:
\begin{align*}
    \vw = \left[ 1, 1, -1, -1, 0 \right]^\top.
\end{align*}
\\
\\
SOLUTION: \\
Based on the specifications of the XOR problem, we know that points within quadrants 1 and 3 should be labeled in the negative class and points within quadrants 2 and 4 should be in the positive class. We also know that the centroids of each quadrant, and thus the basis vectors, are $[-1, 1]^\top, [1, -1]^\top, [1, 1]^\top, [-1, -1]^\top$. The radial basis function is also defined as $rbf(x, r) = \exp\{-(x - r)^2\}$ for all basis vectors. For each vector, this function converges to 1 as x approaches r and converges to 0 as x gets further from r. The output from this function will be a new vector corresponding to $[rbf(x, [-1, 1]^\top), rbf(x, [1, -1]^\top), rbf(x, [1, 1]^\top), rbf(x, [-1, -1]^\top)]$. \\
Since all input vectors are clustered around the basis vectors, then passing the input vectors through the rbf function will result in a one-hot vector corresponding to the quadrant it belongs to. This one-hot vector will be one of the following: \\
$[1, 0, 0, 0]^\top$ in quadrant 2, $[0, 1, 0, 0]^\top$ in quadrant 4, $[0, 0, 1, 0]^\top$ in quadrant 1, and $[0, 0, 0, 1]^\top$ in quadrant 3. However, we want to classify between quadrants 1 and 3 and quadrants 2 and 4. Since we earlier defined that points in quadrants 1 and 3 should be negative, we can negate their rbf vectors such that they are $[0, 0, -1, 0]^\top$ in quadrant 1 and $[0, 0, 0, -1]^\top$ in quadrant 3. The weight vector can be formed as the composition of the rbf of the inputs with the basis vectors and appended with a 0, giving us $\vw = \left[ 1, 1, -1, -1, 0 \right]^\top$. \\
We can then check our weight vector against the outputs from the rbf function. If we take the dot product of $\vw$ with either $[0, 0, 1, 0]^\top$ or $[0, 0, 0, 1]^\top$, we get -1. Similarly if we take the dot product with either $[1, 0, 0, 0]^\top$ or $[0, 1, 0, 0]^\top$ we get 1, which is our desired output and thus proving that this weight vector will perfectly solve the XOR problem.


\vspace{30mm}

\paragraph{2.}

A nearest-neighbour classifier can be constructed as a radial basis
function network by selecting all the input vectors in a training set as basis
vectors. In a multi-class classification setting (i.e., there are more than two
categories), provide a description on how a weight matrix could be built.

\vspace{30mm}

\paragraph{3.} 

Unlike a fixed basis function network, an adaptive basis function network adapts
basis vectors so as to maximize the classification accuracy (i.e. to minimize
the empirical cost.) In order to do so, we need to be able to compute the
gradient of the (logistic regression) distance function with respect to each and
every basis vector. Derive this gradient 
\begin{align*}
    \nabla_{\vr^k} D(y^*, M, \phi(\vx)),
\end{align*}
assuming that $M$ is a logistic regression classifier and that
\begin{align*}
    \phi(\vx) = \left[ 
        \begin{array}{c}
            \exp\left( -(\vx - \vr^1)^2 \right)  \\
            \vdots \\
            \exp\left( -(\vx - \vr^K)^2 \right)  \\
        \end{array}
    \right].
\end{align*}
\\
\\
SOLUTION: \\ 
$D(M(\phi(x)),M,\phi(x)) = -(y^*\log M(\phi(x))+(1-y^*)\log(1-M(\phi(x))))$ \\
This problem is very similar to exercise 3 of homework 1, in which we computed the gradient of the logistic regression distance function for binary classification. For that problem, the gradient was computed as such: \\ 
Let $a = -y^* \log M(\vx)$ and $b =  \log (1- M(\vx))$ \\
We also know that $M(\vx) = \sigmoid(\vw^\top \tilde{\vx}) $ \\
Based off this, we can say $a = -y^* \log ( \frac{1}{1+e^{-\vw^{\top \tilde{\vx}}}})$ and $b =  \log (1- \frac{1}{1+e^{-\vw^\top \tilde{\vx}}})$ \\
$b = \log (1- \frac{1}{1+e^{-\vw^\top \tilde{\vx}}}) = \log ( \frac{1+e^{-\vw^\top \tilde{\vx} } - 1}{1+e^{-\vw^\top \tilde{\vx}}}) = \log ( \frac{e^{-\vw^\top \tilde{\vx} }}{1+e^{-\vw^\top \tilde{\vx}}}) =  \log ( e^{-\vw^\top \tilde{\vx} } ) - \log (1+e^{-\vw^\top \tilde{\vx}}) $ \\
$ \frac{db}{d \vw^\top} = \frac{-\tilde{\vx} e^{-\vw^\top \tilde{\vx}}}{e^{-\vw^\top \tilde{\vx}}} + \frac{\tilde{\vx} e^{-\vw^\top \tilde{\vx}}}{1 + e^{-\vw^\top \tilde{\vx}}}  = \frac{-\tilde{\vx} (1 + e^{-\vw^\top \tilde{\vx}}) +\tilde{\vx} e^{-\vw^\top \tilde{\vx}}}{1 + e^{-\vw^\top \tilde{\vx}}}  = \frac{\tilde{\vx}}{1 + e^{-\vw^\top \tilde{\vx}}}$ \\
$ \frac{da}{d \vw^\top} = \frac {-y^* \tilde{\vx}e^{-\vw^\top \tilde{\vx}}}{1 + e^{-\vw^\top \tilde{\vx}}}$ \\
Bringing this together: \\
$ \nabla_{\vw} D(M^*(\vx), M, \vx) =  \frac {-y^* \tilde{\vx}e^{-\vw^\top \tilde{\vx}}}{1 + e^{-\vw^\top \tilde{\vx}}} - \frac{y^* \tilde{\vx}}{1 + e^{-\vw^\top \tilde{\vx}}} + \frac{\tilde{\vx}}{1 + e^{-\vw^\top \tilde{\vx}}} $ \\
$ = -y^* \tilde{\vx}e^{-\vw^\top \tilde{\vx}} \sigmoid(\vw^\top \tilde{\vx}) -y^* \tilde{\vx}\sigmoid(\vw^\top \tilde{\vx}) + \tilde{\vx}\sigmoid(\vw^\top \tilde{\vx}) = \tilde{\vx}\sigmoid(\vw^\top \tilde{\vx}) (-y^* (1 + e^{-\vw^\top \tilde{\vx}}) + 1)$ \\
$ = \tilde{\vx}\sigmoid(\vw^\top \tilde{\vx}) (\frac{-y^* }{\sigmoid(\vw^\top \tilde{\vx})} + 1) = \tilde{\vx} (-y^* + \sigmoid(\vw^\top \tilde{\vx}))$ \\
The only difference between the computations above and what we're looking for is that we need to substitute $\vx$ for $\phi(x)$. The gradient of $\phi(x)$ with respect to $r^k$ is simply $-2 w_k \phi_k(x)(x-r^k)$. Due to the fact that the derivative of $e^{f(x)}$ is simply $f^{'}(x)e^{f(x)}$, we can simply substitute $\tilde{\vx}$ with the gradient of $\phi(x)$. This gives us: \\
$\nabla_{\vr^k} D(y^*, M, \phi(\vx)) =  (-y^* + \sigmoid(\vw^\top \phi(x)))(2 w_k \phi_k(x)(x-r^k)) \\
= -2(y^* - \sigmoid(\vw^\top \phi(x)))(w_k \phi_k(x)(x-r^k))$
\vspace{30mm}

\paragraph{4.} \todo{PROGRAMMING ASSIGNMENT}





%\bibliographystyle{abbrv}
%\bibliography{../lecture_note}


\end{document}






