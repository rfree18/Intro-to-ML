@article{Hyvarinen2000411,
title = "Independent component analysis: algorithms and applications ",
journal = "Neural Networks ",
volume = "13",
number = "45",
pages = "411 - 430",
year = "2000",
note = "",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(00)00026-5",
url = "http://www.sciencedirect.com/science/article/pii/S0893608000000265",
author = "A. Hyvarinen and E. Oja",
keywords = "Independent component analysis",
keywords = "Projection pursuit",
keywords = "Blind signal separation",
keywords = "Source separation",
keywords = "Factor analysis",
keywords = "Representation "
}

@article{Olshausen19973311,
title = "Sparse coding with an overcomplete basis set: A strategy employed by V1? ",
journal = "Vision Research ",
volume = "37",
number = "23",
pages = "3311 - 3325",
year = "1997",
note = "",
issn = "0042-6989",
doi = "https://doi.org/10.1016/S0042-6989(97)00169-7",
url = "http://www.sciencedirect.com/science/article/pii/S0042698997001697",
author = "Bruno A. Olshausen and David J. Field",
keywords = "Coding",
keywords = "V1",
keywords = "Gabor-wavelet",
keywords = "Natural images "
}

@incollection{NIPS2000_1861,
title = {Algorithms for Non-negative Matrix Factorization},
author = {Daniel D. Lee and Seung, H. Sebastian},
booktitle = {Advances in Neural Information Processing Systems 13},
editor = {T. K. Leen and T. G. Dietterich and V. Tresp},
pages = {556--562},
year = {2001},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf}
}

@article{vincent2010stacked,
  title={Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion},
  author={Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Dec},
  pages={3371--3408},
  year={2010}
}

@article {Hinton504,
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	title = {Reducing the Dimensionality of Data with Neural Networks},
	volume = {313},
	number = {5786},
	pages = {504--507},
	year = {2006},
	doi = {10.1126/science.1127647},
	publisher = {American Association for the Advancement of Science},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/313/5786/504},
	eprint = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
	journal = {Science}
}