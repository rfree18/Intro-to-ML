\documentclass{article}
\usepackage{authblk}
\usepackage{mathptmx}
\usepackage{url,latexsym,amsmath,amsthm,xspace,rotating,multirow,multicol,xspace,amssymb,paralist}
\usepackage{euscript}
\usepackage{fancybox,xcolor}
\usepackage{longtable}
\usepackage{paralist}
\usepackage[normalem]{ulem}
\usepackage[pdftex]{hyperref}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{cancel}
\usepackage{mathtools}

\usepackage{url}
\usepackage{latexsym}

\usepackage{times}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
%\usepackage{hyperref}
\usepackage{url}
%\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{booktabs}
\usepackage{color}
\usepackage[normalem]{ulem}

\newcommand{\obs}{\text{obs}}
\newcommand{\mis}{\text{mis}}

\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\ql}[1]{\left[#1\right]}
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\hl}{HL}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\text{softmax}}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}
\newcommand{\f}{\text{f}}
%\newcommand{\log}{\text{log}}

\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\std}[0]{\operatorname{std}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\ve}[0]{\vect{e}}

\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vp}[0]{\vect{p}}
\newcommand{\vz}[0]{\vect{z}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vi}[0]{\vect{i}}
\newcommand{\vo}[0]{\vect{o}}
\newcommand{\vd}[0]{\vect{d}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\vr}[0]{\vect{r}}
\newcommand{\vone}[0]{\vect{1}}

\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mE}[0]{\matr{E}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mY}[0]{\matr{Y}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mF}[0]{\matr{F}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mC}{\matr{C}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mL}[0]{\matr{L}}
\newcommand{\mR}[0]{\matr{R}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}
\newcommand{\td}[0]{\text{d}}
\newcommand{\TT}[0]{\vects{\theta}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vzero}[0]{\vect{0}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
\newcommand{\YY}[0]{\mathcal{Y}}
\newcommand{\BB}[0]{\mathcal{B}}
\newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\MM}[0]{\mathcal{M}}
\newcommand{\OO}[0]{\mathbb{O}}
\newcommand{\II}[0]{\mathbb{I}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\sign}{\text{sign}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}
\newcommand{\enhnabla}[0]{\nabla_{\hspace{-0.5mm}e}\,}
\newcommand{\eos}[0]{\ensuremath{\left< \text{eos}\right>}}


\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}
\newcommand{\done}[1]{{\Large\textcolor{green}{#1}}}
\newcommand{\dd}[1]{\ensuremath{\mbox{d}#1}}

\DeclareMathOperator*{\argmax}{\arg \max}
\DeclareMathOperator*{\argmin}{\arg \min}
\newcommand{\newln}{\\&\quad\quad{}}

\newcommand{\BP}{\text{BP}}
\newcommand{\PPL}{\text{PPL}}
\newcommand{\PL}{\text{PL}}
\newcommand{\MatSum}{\text{MatSum}}
\newcommand{\MatMul}{\text{MatMul}}
\newcommand{\KL}{\text{KL}}
\newcommand{\data}{\text{data}}
\newcommand{\rect}{\text{rect}}
\newcommand{\maxout}{\text{maxout}}
\newcommand{\train}{\text{train}}
\newcommand{\hinge}{\text{hinge}}
\newcommand{\val}{\text{val}}
\newcommand{\init}{\text{init}}
\newcommand{\fenc}{\text{fenc}}
\newcommand{\renc}{\text{renc}}
\newcommand{\enc}{\text{enc}}
\newcommand{\dec}{\text{dec}}
\newcommand{\test}{\text{test}}
\newcommand{\tra}{\text{tra}}
\newcommand{\Ax}{\mathcal{A}_x}
\newcommand{\Ay}{\mathcal{A}_y}
\newcommand{\ola}{\overleftarrow}
\newcommand{\ora}{\overrightarrow}
\newcommand{\ov}{\overline}
\newcommand{\ts}{\rule{0pt}{2.6ex}}       % Top strut
\newcommand{\ms}{\rule{0pt}{0ex}}         % Middle strut
\newcommand{\bs}{\rule[-1.2ex]{0pt}{0pt}} % Bottom strut
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}


%\usepackage{bibentry}
%\nobibliography*

\begin{document}

\title{Homework Assignment 5}
\author{Lecturer: Kyunghyun Cho}
%\affil{
%    Courant Institute of Mathematical Sciences and \\
%    Center for Data Science,\\
%    New York University 
%}

\maketitle
\pagenumbering{arabic}

\paragraph{1.} 

The probability density function of normal distribution is defined as 
\begin{align*}
    f(\vx) = \frac{1}{Z} \exp\left( 
        -\frac{1}{2} (\vx - \mu)^\top \Sigma^{-1} (\vs - \mu)
    \right),
\end{align*}
where 
\begin{align*}
    Z &= \int_{\vx \in \RR^d} \exp\left( 
        -\frac{1}{2} (\vx - \mu)^\top \Sigma^{-1} (\vs - \mu)
    \right) \dd \vx \\
    &= (2\pi)^{-d/2} |\Sigma|^{-1/2},
\end{align*}
where $|\Sigma|$ is the determinant of the covariance matrix.

Let us assume that the covariance matrix $\Sigma$ is a diagonal matrix, as
below:
\begin{align*}
    \Sigma = \left[
        \begin{array}{c c c c}
            \sigma_1^2 & 0 & \cdots & 0 \\
            0 & \sigma_2^2 & \cdots & 0 \\
            \vdots & 0 & \cdots & 0 \\
            \vdots & \vdots & \cdots & \vdots \\
            0 & 0 & \cdots & \sigma_d^2 
        \end{array}
    \right].
\end{align*}
The probability density function simplifies to
\begin{align*}
    f(\vx) = \prod_{i=1}^d 
    \frac{1}{\sqrt{2\pi} \sigma_i}
    \exp\left(
        -\frac{1}{2} \frac{1}{\sigma_i^2}(x_i - \mu_i)^2
    \right).
\end{align*}

Show that this is indeed true. \\
\\
SOLUTION:\\
$Z = (2\pi)^{-d/2} |\Sigma|^{-1/2}$
$|\Sigma| = \prod_{i = 1}^d \sigma_i^2$ \\
$Z = (2\pi)^{-d/2} (\prod_{i = 1}^d \sigma_i^2)^{\frac{-1}{2}}$ \\
$= (2\pi)^{-d/2}  \prod_{i = 1}^d \frac{1}{\sigma_i}$ \\
$= \prod_{i = 1}^d \frac{1}{\sqrt{2 \pi} \sigma_i}$ \\
\\
$f(\vx) = \frac{1}{Z} \exp\left( 
        -\frac{1}{2} (\vx - \mu)^\top \Sigma^{-1} (\vx - \mu)
    \right)$ \\
$ = \prod_{i = 1}^d \frac{1}{\sqrt{2 \pi} \sigma_i} \exp\left( 
        -\frac{1}{2} (\vx - \mu)^\top \Sigma^{-1} (\vx - \mu)
    \right)$ \\
$ = \prod_{i = 1}^d \frac{1}{\sqrt{2 \pi} \sigma_i} \exp\left( 
        -\frac{1}{2} (x_i - \mu_i) \Sigma^{-1} (x_i - \mu_i)
    \right)$ \\
$ = \prod_{i=1}^d 
    \frac{1}{\sqrt{2\pi} \sigma_i}
    \exp\left(
        -\frac{1}{2} \frac{1}{\sigma_i^2}(x_i - \mu_i)^2
    \right) $
%
\vspace{30mm}

\paragraph{2.}

(a) Show that the following equation, called Bayes' rule, is true.
\begin{align*}
    p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)}.
\end{align*}
\\
SOLUTION: We know that $p(X, Y) = p(X|Y)p(Y)$ and that $p(Y|X) = \frac{p(X, Y)}{p(X)}$\\
Therefore: \\
$p(Y|X) = \frac{p(X, Y)}{p(X)} = \frac{p(X|Y)p(Y)}{p(X)}$

\vspace{15mm}

(b) We learned the definition of expectation:
\begin{align*}
    \E\left[ X \right] = \sum_{x \in \Omega} x p(x). 
\end{align*}

Assuming that $X$ and $Y$ are discrete random variables, show that
\begin{align*}
    \E\left[ X + Y \right] = \E\left[X\right] + \E\left[Y \right].
\end{align*}
\\
SOLUTION: \\
$\E\left[ X + Y \right] = \sum_{x, y \in \Omega} (x + y) p(x, y) = \sum_{x, y \in \Omega} x p(x, y) + \sum_{x, y \in \Omega} y p(x, y)$ \\
$= \sum_{x \in \Omega} x \sum_{y \in \Omega} p(x, y) + \sum_{y \in \Omega} y \sum_{x \in \Omega} p(x, y)$ \\
$= \sum_{x \in \Omega} x p(x) + \sum_{y \in \Omega} y p(y)$ \\
$= \E\left[X\right] + \E\left[Y \right]$
\vspace{15mm}

(c) Further assume that $c \in \RR$ is a scalar and is not a random variable, show
that
\begin{align*}
    \E\left[ cX \right] = c\E\left[X\right].
\end{align*}
\\
$\E\left[ cX \right] = \sum_{x \in \Omega} cx p(x) = c \sum_{x \in \Omega} x p(x) = c\E\left[X\right] $
\vspace{15mm}

(d) We learned the definition of variance:
\begin{align*}
    \text{Var}(X) = \sum_{x \in \Omega} (x - \E\left[ X \right])^2 p(x).
\end{align*}
\\
$\text{Var}(X) = \sum_{x \in \Omega} (x - \E\left[ X \right])^2 p(x)$
Assuming $X$ being a discrete random variable, show that
\begin{align*}
    \text{Var}(X) = \E\left[ X^2 \right] - \left(\E\left[ X \right]\right)^2.
\end{align*}
\\
SOLUTION: \\
$ \text{Var}(X) = \sum_{x \in \Omega} (x - \E\left[ X \right])^2 p(x)$ \\
$ = \sum_{x \in \Omega} (x^2 - 2x \E\left[ X \right] + (\E\left[ X \right])^2) p(x)$ \\
$= \sum_{x \in \Omega} x^2 p(x) - \sum_{x \in \Omega} 2x \E\left[ X \right] p(x) + \E\left[ X \right]^2 $ \\
$ = \E\left[  x^2 \right] - 2 \E\left[ X \right] \sum_{x \in \Omega} x  p(x) + \E\left[ X \right]^2$ \\
$= \E\left[  x^2 \right] - 2 \E\left[ X \right]^2 + \E\left[ X \right]^2$ \\
$= \E\left[  x^2 \right] - \E\left[ X \right]^2$
\newpage

\paragraph{3.} 

(a) An optimal linear regression machine (without any regularization term), that
minimizes the empirical cost function given a training set 
\begin{align*}
    D_{\tra} = \left\{ 
        (\vx_1, \vy_1^*), \ldots, (\vx_N, \vy_N^*)
    \right\},
\end{align*}
can be found directly without any gradient-based optimization algorithm.
Assuming that the distance function is defined as 
\begin{align*}
    D(M^*(\vx), M, \vx) = \frac{1}{2} \|M^*(\vx) - M(\vx)\|_2^2 = 
    \frac{1}{2} \sum_{k=1}^q (y^*_k - y_k)^2,
\end{align*}
derive the optimal weight matrix $\mW$. (Hint: Moore–Penrose pseudoinverse)


\vspace{15mm}

(b) (Extra Credit) Derive a  probability density function of the predictive distribution of
Bayesian linear regression. Follow the assumptions made during the lecture (see
the lecture note.) 








%\bibliographystyle{abbrv}
%\bibliography{../lecture_note}


\end{document}






